#### 为什么要对数据进行降维？

&emsp; &emsp; 1. 降维可以缓解维度灾难问题。

&emsp; &emsp;2. 降维可以在压缩数据的同时让信息损失最小化。

&emsp; &emsp;3. 理解几百个维度的数据结构很困难，两三个维度的数据通过可视化更容易理解。

&emsp; &emsp;4，降低很多算法的开销，去除噪声，使得数据集更易用

#### 对于PCA（Principal Component Analysis，主成分分析）的理解:PCA通过线性变换将原始数据变换为一组各维度*线性无关*的表示，

可用于提取数据的主要特征分量，常用于高维数据降维。直白的来说，就是对一个样本矩阵:

&emsp;（1）换特征，找一组新的特征来重新表示

&emsp;（2）减少特征，新特征的数目要远小于原特征的数目

&emsp;####  1、什么是主成分？

&emsp;矩阵的主成分是由其协方差矩阵的特征向量，按照对应的特征值大小排序得到的。最大的特征值就是第一主成分，第二大的特征值就是第二主成分，以此类推。

&emsp; #### 2、主成分分析方法为什么能够降维？

&emsp;主成分分析就是找一个新的矩阵（也就是一组基底的合集），让样本矩阵乘以这个矩阵，实现换特征+减少特征的重新表示。

这个基底有要求的，

&emsp;（1）它要使得原始数据集经过这个正交基底变换后，尽可能的分散，这个分散就是样本在这个“基底上的坐标”（这个基底上的特征值）的方差要尽可能大

&emsp;（2）使得各个选择的基底关联程度最小 。让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。所以最好就是选择和第一个基底正交的基底。

对于协方差矩阵的特征值和特征向量，可以这么理解，特征值是数据集变换后方差的大小，特征向量是是数据集进行变换后空间的坐标基底（也就是正交基）

&emsp;  协方差（Covariance）是度量两个变量的变动的同步程度，也就是度量两个变量线性相关性程度。我们进行数据降维是为了降低数据的维度，同时保留含有信息量大的数据。

数据的信息量可以用数据的方差来衡量。在给定的数据集中，其协方差矩阵可以衡量给定数据集各个维度之间的相关程度，那么我们希望数据集的协方差矩阵对角线元素尽量的大，
那么所包含的信息量不就大了嘛，非对角元素尽量的小，最好为0。



那么求得了协方差矩阵的特征值和特征向量就可以对数据进行降维了

#### PCA实现的流程

&emsp; 将数据转换成前N个主成分的伪代码大致如下:

&emsp; &emsp; 去除平均值

&emsp; &emsp; 计算数据集的协方差矩阵

&emsp; &emsp; 计算协方差矩阵的特征值和特征向量

&emsp; &emsp; 将特征值从大到小排序

&emsp; &emsp; 保留上面的前N个特征向量

&emsp; &emsp; 将数据转换到上述N个特征向量构建的新空间中

#### 那么问题来了。

1、为什么要去除各个维度的平均值？

&emsp; 样本矩阵中心化以后，样本均值为0，因此式协方差公式中每个维度无需减去均值，只需要进行与其他维度的乘法，这样就可以用转置相乘实现任意两两维度的相乘。

#### “将数据转换到上述N个特征向量构建的新空间中”------->矩阵相乘的“变换的本质”理解

&emsp;A*B两个矩阵相乘代表什么？

&emsp;A的每一行所表示的向量，变到B的所有列向量为基底表示的空间中去，得到的每一行的新的表示。

&emsp;B的每一列所表示的向量，变到A的所有行向量为基底表示的空间中去，得到的每一列的新的表示。